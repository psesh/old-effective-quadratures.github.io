Nataf transform for correlated inputs
========================================
In most applications of polynomial approximations, one typically assumes that the input parameters are independent. This independence assumption underpins the computation of moments. In a nutshell, the notion of independence permits us to express the joint probability distribution of the inputs as a product of its composite marginals, i.e., 

.. math::
	\boldsymbol{\rho}\left(\boldsymbol{\zeta}\right)=\rho_{1}\left(\zeta_{1}\right)\ldots\rho_{d}\left(\zeta_{d}\right)

where :math:`\rho_{i}\left(\zeta_{i}\right)` for :math:`i=1, \ldots, d` represents the PDF associated with variable :math:`\zeta_{i}`. However, in many application areas, this assumption is not true; the inputs may be correlated with known linear rank correlation values. While one can still use Monte Carlo techniques to generate samples from correlated spaces, it is not straightforward to generate quadrature points from such spaces. This sets the stage for the Nataf transform [1].

The Nataf transformation transforms a set of correlated random variables to a set of uncorrelated standard normal random variables. To perform a Nataf transformation one requires:

- The definition of the marginals :math:`\rho_{i}\left(\zeta_{i}\right)`;
- The linear correlation matrix between the marginals :math:`\mathbf{P}`.

This implies that the polynomials are constructed in the space of uncorrelated standard normal random variables---i.e., Hermite polynomials with Gauss-Hermite quadrature points [2]. However, the function evaluations are carried out in the correlated space of random variables using the Nataf transformation. The transform has two steps:

**Correlated random space to correlated standard space:**
Let :math:`\boldsymbol{\zeta}=\left\{ \zeta_{1},\ldots,\zeta_{d}\right\}` be a set of random variables and let :math:`\boldsymbol{\theta}=\left\{ \theta_{1},\ldots,\theta_{d}\right\}` denote random variables form the *standard multivariate normal* :math:`\mathcal{N}\left( \boldsymbol{0}, \mathbf{I} \right)`. One can then express 

.. math::

	\theta_{i}=\Phi^{-1}\left[F_{i}\left(\zeta_{i}\right)\right],


where :math:`F_{i}` is marginal cumulative density function of :math:`\zeta_{i}` and :math:`\Phi^{-1}` is the inverse CDF of a standard normal distribution.

**Correlated standard space to uncorrelated standard space:**
Here the Cholesky factor :math:`\mathbf{L}` of a *modified* correlation matrix :math:`\mathbf{P}^{\ast}` is used to transform the variables :math:`\boldsymbol{\theta}` into :math:`\boldsymbol{u}`

.. math::
	
	\boldsymbol{\theta} = \mathbf{L} \boldsymbol{u},

where :math:`\boldsymbol{u}=\left\{ u_{1},\ldots,u_{d}\right\}` is a set of uncorrelated normal random variables. The key difficulty in the Nataf transformation is the computation of :math:`\mathbf{P}^{\ast}` given :math:`\mathbf{P}`. It boils down to approximating the integral equation

.. math::
	
	p_{ij}=\frac{1}{\sigma_{\zeta_{i}}\sigma_{\zeta_{j}}}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}F_{i}^{-1}\left(\Phi\left(\theta_{i}\right)-\mu_{\zeta_{i}}\right)F_{j}^{-1}\left(\Phi\left(\theta_{j}\right)-\mu_{\zeta_{j}}\right)\mathcal{N}\left(0,p_{ij}^{\ast}I\right)d\theta_{i}d\zeta_{j}

where 

- :math:`p_{ij}` is the (i,j)-th entry of :math:`\mathbf{P}`;
- :math:`p_{ij}^{\ast}` is the (i,j)-th entry of :math:`\mathbf{P}^{\ast}`;
- :math:`\sigma_{\zeta_{i}}` is the standard deviation of the parameter :math:`\zeta_{i}`;
- :math:`\mu_{\zeta_{i}}` is the mean of the parameter :math:`\zeta_{i}`;
- :math:`\mathcal{N}\left(0,p_{ij}^{\ast}I\right)` is a bivariate, zero-mean normal distribution with a variance of 1 and a correlation of :math:`p_{ij}^{\ast}`.

In practice this integral equation is difficult to solve analytically for :math:`p_{ij}^{\ast}`, thus an iterative root-finding strategy based on Newton's method is adopted in Effective Quadratures. 

Now, to demonstrate this capability, consider the following problem of estimating statistical moments. Consider a *blackbox* model of the form

.. math::
	
	f \left( \boldsymbol{\zeta} \right) = \zeta_{1} + \zeta_{2} + \zeta_{3} - 12,

where the input parameters :math:`\left( \boldsymbol{\zeta} \right)` are defined as follows. 

.. code::

	from equadratures import *
	import numpy as np 
	import matplotlib.pyplot as plt
	from scipy.stats import skew 

	zeta_1 = Parameter(distribution='truncated-gaussian', shape_parameter_A = 3.0, shape_parameter_B = 2.0, order=3, lower=-2.0, upper=4.0)
	zeta_2 = Parameter(distribution='truncated-gaussian', shape_parameter_A = -1.0, shape_parameter_B = 0.1, order=3, lower=-5.0, upper=5.0)
	zeta_3 = Parameter(distribution='truncated-gaussian', shape_parameter_A = 2.0, shape_parameter_B = 2.0, order=3, lower=0.0, upper=6.0)

	def fun(x):
    		return 5.0 * x[0]**3 - x[0]*x[1] + 3.0*x[1]*x[2]**3 + 32.0

Plots of these parameters are shown below. In practice, one can opt for any of the distributions supported by the Parameter class.

.. figure:: Figures/Fig_A.png
   :scale: 30 %

   Figure. Parameter :math:`\zeta_{1}`.


.. figure:: Figures/Fig_B.png
   :scale: 30 %

   Figure. Parameter :math:`\zeta_{2}`.

.. figure:: Figures/Fig_C.png
   :scale: 30 %

   Figure. Parameter :math:`\zeta_{3}`.

This sorts out the marginals. We now define our linear rank correlation matrix

.. math::

	\mathbf{P}=\left[\begin{array}{ccc} 1.0 & 0.6 & 0.3\\ 0.6 & 1.0 & 0.2\\ 0.3 & 0.2 & 1.0 \end{array}\right]

.. code::

	R = np.eye(3)
	R[0, 1] = 0.6
	R[0, 2] = 0.3
	R[2, 1] = 0.2 
	R[1, 0] = R[0, 1]
	R[2, 0] = R[0, 2]
	R[1, 2] = R[2, 1]

Now, we set up the machinery required computing this transform. We shall compare our result with Monte Carlo. 

.. code::

	u1 = Parameter(distribution='normal', shape_parameter_A=0.0, shape_parameter_B=1.0, order=1)
	myNataf = Nataf([zeta_1, zeta_2, zeta_3], P)
	
	# For Monte-Carlo!
	samples_mc = myNataf.getCorrelatedSamples(N=10000)
	f_mc = evalfunction(samples_mc, fun)

	# For Polynomials!
	myBasis = Basis('Tensor grid')
	myPoly = Polyint([u1, u1, u1], myBasis)
	samples_p =  myPoly.quadraturePoints
	samples_corr_p = myNataf.U2C(samples_p)
	f_p = evalfunction(samples_corr_p, fun)

As before, we use the get statistics utility to estimate moments associated with this polynomial approximation:

.. code::

	myPoly.computeCoefficients(f_p)
	myStats = myPoly.getStatistics()

The plot below shows the quadrature points and the random Monte Carlo samples.

.. code::

	fig = plt.figure()
	ax = fig.add_subplot(111, projection='3d')
	ax.scatter(samples_mc[:,0], samples_mc[:,1], samples_mc[:,2], marker='o', s=5, c='dodgerblue', alpha=0.6)
	ax.scatter(samples_corr_p[:,0], samples_corr_p[:,1], samples_corr_p[:,2], marker='o', s=80, c='darkorange', linewidths=1, edgecolors='k')
	ax.set_xlabel('$\zeta_{1}$', fontsize=12)
	ax.set_ylabel('$\zeta_{2}$', fontsize=12)
	ax.set_zlabel('$\zeta_{3}$', fontsize=12)
	ax.view_init(40, -41)
	plt.savefig('Fig_D.png', dpi=200)

.. figure:: Figures/Fig_D.png
   :scale: 30 %

   Figure. Random correlated samples (shown in blue) and quadrature points associated with the isotropic tensor grid with 3 points in each direction.


Now, lets compare statistics. Note, we've set the order of the parameter :code:`u1` parameter to be 1; this can be increased. Setting the number of Monte Carlo samples :code:`N=50000`, we obtain:

.. code::
	
	print '----MONTE CARLO----'
	print np.mean(f_mc), np.var(f_mc), skew(f_mc)

	print '----POLYNOMIALS-----'
	print myStats.mean, myStats.variance, myStats.skewness

	----MONTE CARLO----
	-8.35251940491 3.91447723802 -0.10088043
	----POLYNOMIALS-----
	-8.36876731443 4.24868942164 0.0630390307101

Now, if we increase the order from 1 to 3, we obtain:

.. code::

	----MONTE CARLO----
	-8.35251940491 3.91447723802 -0.10088043
	----EQ-----
	-8.36091830994 3.95592725014 -0.105047765166

**References**

.. [1] Lebrun, R., Dutfoy, A. An innovating analysis of the Nataf transformation from the copula viewpoint. Probabilistic Engineering Mechanics 24.3 (2009): 312-320. `Paper <https://www.sciencedirect.com/science/article/pii/S0266892008000660>`__

.. [2] Eldred, M., Webster, C., Constantine, P. Evaluation of non-intrusive approaches for Wiener-Askey generalized polynomial chaos. 49th AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference, 16th AIAA/ASME/AHS Adaptive Structures Conference, 10th AIAA Non-Deterministic Approaches Conference, 9th AIAA Gossamer Spacecraft Forum, 4th AIAA Multidisciplinary Design Optimization Specialists Conference, 2008. `Paper <https://arc.aiaa.org/doi/abs/10.2514/6.2008-1892>`__
