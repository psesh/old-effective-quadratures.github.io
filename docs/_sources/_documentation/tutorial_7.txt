Bayesian polynomial regression
==============================================

In this tutorial we present a Bayesian analogue to polynomial regression; the material presented here is heavily derived  from Chapter 3.8 in Rogers and Girolami. We will aim to fit a Bayesian polynomial regression model to the data we considered in the prior tutorial. Bayesian approaches are characterized by four key elements: the model, the prior, the likelihood and the posterior.

**The model**

We define our model as

.. math::

  \mathbf{f} = \mathbf{Ax} + \boldsymbol{\epsilon},

where :math:`\mathbf{A} \in \mathbb{R}^{N \times M}` is our *Vandermonde* type matrix, comprising of :math:`M` orthogonal polynomials evaluated at the :math:`N` *sample inputs*. In other words each entry of :math:`\mathbf{A}` can be given by

.. math::

  \mathbf{A}_{ij} = \psi_i \left(  \lambda_j \right).

Returning back to the first equation, we have :math:`\boldsymbol{\epsilon} \in \mathbb{R}^{N}` which is the noise associated with each *sample output*, i.e., each element of the vector. To simplify matters, we introduce the standard Gaussian assumption for the noise

.. math::

   \boldsymbol{\epsilon} = \mathcal{N} \left( \boldsymbol{0}, \boldsymbol{\Sigma} \right),

where :math:`\boldsymbol{0}` indicates the zero vector and :math:`\boldsymbol{\Sigma}` denotes the covariance matrix associated with the noisy measurements. We assume that these are known. Infact we will assume that

.. math::

  \boldsymbol{\Sigma} = \sigma^2 \mathbf{I}

where :math:`\mathbf{I}` is the identity matrix; we set :math:`\sigma=1.5`.

**The likelihood**

In a Bayesian context, the likelihood can be interpreted as a model of some parameters given data. It is formed from the joint probability of both the same data and its associated model parameters. Our likelihood function here is given by

.. math::

  p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) = \mathcal{N} \left( \mathbf{Ax}, \sigma^2 \mathbf{I}     \right).


**The prior**

One of the goals of polynomial regression is to ascertain the value of our coefficients :math:`\mathbf{x}`. As we would like an exact expression for the posterior, as Rogers and Girolami state, we require a prior that is **conjugate** to the Gaussian likelihood above. The simplest choice for facilitating this conjugacy is to ensure that our prior too is Gaussian. We therefore write our prior as

.. math::

  p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right) = \mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right),

where we set :math:`\boldsymbol{\mu} = \boldsymbol{0}` and :math:`\boldsymbol{\Sigma}_{0} = 10 \mathbf{I}`. The high-level idea is that we generate random coefficients---sampling from :math:`\mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right)`---for the first six Legendre polynomials and plot them!

.. code::

  p_order = 5
  myBasis = Basis('Univariate')
  poly = Poly(myParameters, myBasis)

  P = poly.get_poly(x_test).T # Extracting the Vandermonde-type matrix!

  # Define the prior!
  mu_0 = np.zeros((p_order+1))
  Sigma_0 = 100. * np.eye(p_order+1)
  samples = 300
  coefficients_dist = np.random.multivariate_normal(mu_0, Sigma_0, (samples,p_order+1))

.. figure:: Figures/tutorial_7_fig_a.png
   :scale: 40 %

   Figure. Samples from the prior distribution (grey lines) assigned to the polynomial coefficients.


**The posterior**

But clearly this isn't too informative. What we want is to posterior! The posterior can be expressed as a product of the likelihood and the prior. In this tutorial, we know our posterior will also be a Gaussian distribution. What is not known is its precise mean and covariance. Using Bayes' rule, one can express the posterior distribution as

.. math::

  p \left( \mathbf{x} | \mathbf{f}, \mathbf{A}, \sigma^2  \right) \propto  p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right),

which upon simplification yields

.. math::

  = \text{exp} \left\{  -\frac{1}{2} \left( \frac{1}{\sigma^2} \left( \mathbf{f} - \mathbf{Ax}  \right)^{T}  \left( \mathbf{f} - \mathbf{Ax}  \right)  +   \left( \mathbf{x} - \boldsymbol{\mu} \right)^{T}  \boldsymbol{\Sigma}_{0}^{-1}  \left( \mathbf{x} - \boldsymbol{\mu} \right)     \right)     \right\}.

The covariance and the mean can then be given as

.. math::

  \boldsymbol{\Sigma}_{\mathbf{x}} = \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{A}  + \boldsymbol{\Sigma}^{-1}_{0}    \right)^{-1}

.. math::

    \boldsymbol{\mu}_{\mathbf{x}} = \boldsymbol{\Sigma}_{\mathbf{x}} \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{f} + \boldsymbol{\Sigma}_{0}^{-1} \boldsymbol{\mu}_{0}   \right)

respectively.

.. code::

  values = np.array([0,2,9, 11]) # only selecting 4 sample points!
  x_use = x_train[values]
  y_use = y_train[values]
  Sigma_measurements = noise**2 * np.eye(len(values))
  P_data = poly.get_poly(x_use).T
  # and now computing the posterior covariance and mean...
  Sigma_x = np.linalg.inv( P_data.T @ np.linalg.inv(Sigma_measurements) @ P_data  + np.linalg.inv(Sigma_0) )
  mu_x = Sigma_x @   P_data.T @ np.linalg.inv(Sigma_measurements) @ y_use
  coefficients_dist = np.random.multivariate_normal(mu_x.flatten(), Sigma_x, (samples,p_order+1))

.. figure:: Figures/tutorial_7_fig_b.png
   :scale: 40 %

   Figure. Samples created from the coefficients drawn from the posterior after observing four data points. The mean is shown in blue.

The full source code for this tutorial can be found `here. <https://github.com/Effective-Quadratures/Effective-Quadratures/blob/master/source/_documentation/codes/tutorial_7.py>`__


**References**

- Rogers, S., Girolami, M. (2016). A First Course in Machine Learning. Chapman and Hall/CRC, Boca Raton, Florida, U.S.A.
