Active subspaces with polynomial approximations
=================================================

**Active Subspaces**

The active subspace is defined as follows: 

Let :math:`f(\mathbf{x})` be a scalar valued function :math:`\mathbb{R}^n \rightarrow \mathbb{R}` with :math:`\mathbf{x} \in \mathbb{R}^{n}`. Here :math:`f(\mathbf{x})` represents the chosen quantity of interest and :math:`\mathbf{x}` represents the design parameters. Consider the approximation

.. math::

        \begin{equation}
        f(\mathbf{x}) \approx g( \mathbf{U}^T \mathbf{x}),
        \label{equ_active}
        \end{equation}

where :math:`\mathbf{U} \in \mathbb{R}^{k \times n}`, with :math:`k << n`. Ideally we would like :math:`k = 1` or :math:`2` for facilitating subsequent visualisation. The matrix :math:`\mathbf{U}` represents the *active subspace* and its null space represents the *inactive subspace*. Essentially, the function tends to be well approximated via the map :math:`g` on the active subspace.

Numerous ideas for finding active subspaces in computer experiments can be found in the work of Constantine [1]. The idea is to express the kernel matrix as the expectation of the outer product of gradients of :math:`f`

.. math::

        \begin{equation}
        \textbf{K}=\int \nabla_{\mathbf{x}} f(\mathbf{x})\nabla_{\mathbf{x}} f(\mathbf{x})^{T} \rho (\mathbf{x}) d \mathbf{x} ,
        \label{equ_as}
        \end{equation}

where :math:`\rho(\mathbf{x})` is the density of the inputs :math:`\mathbf{x}` over its parameter space. A heuristic for estimating the kernel via finite point samples :math:`N` is described below:

1.  For :math:`N` samples drawn randomly from a distribution :math:`\rho(\mathbf{x})` calculate the sample kernel matrix:

.. math::

        \begin{equation}
        \hat{\textbf{K}}=\frac{1}{N} \sum_{i=1}^{N}(\nabla_{\mathbf{x}} f(\mathbf{x}_i))(\nabla_{\mathbf{x}} f(\mathbf{x}_i)^{T})\,;
        \end{equation}

2. Compute the eigendecomposition of :math:`\hat{\textbf{K}}` and select the eigenvectors with largest eigenvalues to form a matrix :math:`\mathbf{U}` with orthogonal columns.

**Polynomial Active Subspaces**

In many cases gradient information is not available. Therefore, assumptions on the model are necessary to overcome this obstacle. A good assumption would be that the model is a polynomial, as the polynomials are easy to calculate and their gradients can be derived analytically. One can express a polynomial as a linear combination of a number of polynomial basis terms

.. math::

        \begin{equation}
        f(\mathbf{x}) \approx h(\mathbf{x})=\sum_{j=1}^N a_j \phi_j(\mathbf{x})\,,
        \end{equation}

where :math:`\phi_j(\mathbf{x})` is a multivariate polynomial basis term of a certain mixed degree and :math:`a_j` is the corresponding coefficient. Then the gradient can be expressed as the sum of the derivatives of the polynomials.

.. math::

        \begin{equation}
        \nabla_{\mathbf{x}}f(\mathbf{x}) \approx \nabla_{\mathbf{x}}h(\mathbf{x})=\sum_{j=1}^N a_j \phi_j'(\mathbf{x})\,.
        \end{equation}

To construct the polynomial basis terms :math:`\phi(\mathbf{x})` , one only needs to know the number of dimensions and the polynomial degree. 

**A 2-degree Polynomial Model for Dimension Reduction**

If the polynomial degree is set as 2, then the polynomial bases would include all linear terms :math:`\phi(\mathbf{x})=x_i`, all squared terms :math:`\phi(\mathbf{x})=x_i^2` and all cross terms :math:`\phi(\mathbf{x})=x_i x_j`.

One can fit a polynomial to the dataset using least squares with sample input/output pairs. This idea was leveraged in [2] to find :math:`\mathbf{U}` and is detailed below. The goal here is to use least squares to fit the dataset to the following form

.. math::

        \begin{equation}
        y=f(\mathbf{x})=\frac{1}{2}\mathbf{x} ^{T} \mathbf{A}\mathbf{x} + \textbf{c}^{T}\mathbf{x} +d\,,
        \label{quadraticeqn}
        \end{equation}

therefore the gradient of the function can be expressed as

.. math::

        \begin{equation}
        \nabla_{\mathbf{x}}f(\mathbf{x})= \mathbf{A} \mathbf{x}+\textbf{c}\,.
        \end{equation}

Then a set of equations can be formulated using each data point:

.. math::

        \begin{equation}
        \frac{1}{2} \sum_{i=1,j=1}^{m,m}a_{ij}x_ix_j+\sum_{j=1}^{m}c_jx_j+d=y\,;
        \end{equation}

All :math:`a_{ij}` along with :math:`\textbf{c}` and :math:`d` are obtained by solving for the least squares solution, where :math:`a_{ij}` are the entries of the matrix :math:`\mathbf{A}` and :math:`c_j` are the entries of the vector :math:`\textbf{c}`. Finally, the estimated kernel is given by

.. math::
    
        \begin{equation}
        \hat{\textbf{K}}=\frac{1}{N}\sum_{i=1}^N(A\mathbf{x}_i+\textbf{c})(A\mathbf{x}_i+\textbf{c})^{T}\,.
        \end{equation}

The last step of selecting eigenvectors is done by carrying out an eigendecomposition on the kernel matrix (symmetric).

.. math::

        \begin{equation}
        \hat{\mathbf{K}}= \mathbf{W \Lambda W}^{T},
        \end{equation}

where :math:`\mathbf{W}` contains all eigenvectors and :math:`\Lambda` is the diagonal matrix of all corresponding eigenvalues, which are assumed to be sorted in descending order. The eigenvectors and eigenvalues can then be partitioned

.. math::

        \begin{equation}
        \mathbf{W}=\left[\begin{array}{cc}
        \mathbf{U}_{\text{active}} & \mathbf{U}_{\text{inactive}}\end{array}\right] \; \; \; \mathbf{\Lambda}=\left[\begin{array}{cc}
        \mathbf{\Lambda_{\text{active}} }\\
         & \mathbf{\Lambda_{\text{inactive}} }
        \end{array}\right]
        \end{equation}

based on a *suitable cut-off criterion* ---usually the magnitude of the eigenvalues.

As most of the variation in :math:`f` happens with changes in :math:`\mathbf{U}_{active}^T \mathbf{x}`, 

.. math::

        \begin{equation}
        f \left(\mathbf{x}\right) \approx g\left(\mathbf{U}_\text{active}^T\mathbf{x} \right)\text{ while }f\left( \mathbf{U}_\text{inactive} \mathbf{U}_\text{inactive}^T \mathbf{x} \right) \approx 0,
        \end{equation}

where :math:`g` is a lower dimensional function.

**Code Implementation**

We look at a compressor fan design problem. The 25 design variables can be viewed as inputs to the function, where the CFD efficiency of the blade design is seen as the output. Now we attempt to use a 2-degree polynomial active subspace model to reach dimension reduction and find a 2D active subspace.

.. code::

        from equadratures import *
	import numpy as np
	import matplotlib.pyplot as plt
	from scipy.spatial import ConvexHull, convex_hull_plot_2d
	from mpl_toolkits.mplot3d import axes3d

.. code::

        X = np.loadtxt('../data/design_parameters.dat')
	y = np.loadtxt('../data/non_dimensionalized_efficiency.dat')
	title = 'Normalised efficiency'

This dataset contains 554 pairs of inputs (designs of blades) and outputs (CFD-evaluataed efficiency). The dimension of inputs is 25 (25D design space). All input vectors are in X , while the outputs are in y.

The first order of business is to construct a global quadratic model

.. code::

        s = Parameter(distribution='uniform', lower=-1., upper=1., order=2)
	myparameters = [s for _ in range(0, 25)]
	mybasis = Basis('total-order')
	mypoly = Poly(parameters=myparameters, basis=mybasis, method='least-squares', \
              sampling_args= {'mesh': 'user-defined', 'sample-points': X, 'sample-outputs': y})
	mypoly.set_model()

Next, we compute the active subspace


.. code::

        mysubspace = Subspaces(full_space_poly=mypoly, method='active-subspace')
	W = mysubspace.get_subspace()
	e = mysubspace.get_eigenvalues()

and plot the eigenvalues to determine how many *active* variables we require.

.. code::

        fig = plt.figure(figsize=(7,4))
	ax = fig.add_subplot(111)
	plt.semilogy(e, 'o')
	plt.ylabel('Eigenvalues (log-scale)')
	plt.xlabel('Design parameters')

.. figure:: Figures/tutorial_11_fig_a.png
   :scale: 30 %

We can then project all the samples we have within the design space onto the first few dimensions of the active subspace.

.. code::
	
	true_dimensions = 1
	u = X @ W[:, 0:true_dimensions]
	fig = plt.figure(figsize=(7,4))
	ax = fig.add_subplot(111)
	plt.plot(u[:,0], y, 'o', color='gold', markeredgecolor='k', lw=1, ms=13, alpha=0.8)
	plt.ylabel(title)
	plt.xlabel('u')

.. figure:: Figures/tutorial_11_fig_b.png
   :scale: 30 %

And for two dimensions

.. code::

	true_dimensions = 2
	u = X @ W[:, 0:true_dimensions]
	fig = plt.figure(figsize=(10,10))
	ax = fig.add_subplot(111, projection='3d')
	ax.scatter(u[:,0],u[:,1], y, s=50, c=y, marker='o', edgecolor='k', lw=1, alpha=0.8)
	ax.set_xlabel('u1')
	ax.set_ylabel('u2')
	ax.set_zlabel(title)

.. figure:: Figures/tutorial_11_fig_c.png
   :scale: 30 %

Finally, we can also compute the zonotope associated with the 2D projection.

.. code::

	z = X @ W[:,0:true_dimensions]
	pts = mysubspace.get_zonotope_vertices()
	hull = ConvexHull(pts)
	fig = plt.figure(figsize=(11,8))
	ax = fig.add_subplot(111)
	plt.plot(z[:,0], z[:,1], 'o', color='gold', markeredgecolor='k', lw=1, ms=13, alpha=0.8)
	for simplex in hull.simplices:
    		plt.plot(pts[simplex, 0], pts[simplex, 1], 'k-', lw=2)

.. figure:: Figures/tutorial_11_fig_d.png
   :scale: 30 %

The full source code for this tutorial can be found `here. <https://github.com/Effective-Quadratures/Effective-Quadratures/blob/master/source/_documentation/codes/tutorial_11.py>`__

**References**

-  Constantine, P. G. (2015) Active subspaces: Emerging ideas for dimension reduction in parameter studies, Volume 2, SIAM, 2015.

- Seshadri, P., Shahpar, S., Constantine, P., Parks, G., Adams, M. (2018) Turbomachinery active subspace performance maps. Journal of Turbomachinery, 140(4), 041003. `Paper <http://turbomachinery.asmedigitalcollection.asme.org/article.aspx?articleid=2668256>`__

