<!DOCTYPE html>
<html >
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
      <title>Bayesian polynomial regression</title>
    
      <link rel="stylesheet" href="../_static/pygments.css">
      <link rel="stylesheet" href="../_static/theme.css">
      <link rel="stylesheet" href="../_static/sphinx_press_theme.css">
          <link rel="stylesheet" href="../_static/styles.css" type="text/css" />
      
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>

      <!-- sphinx script_files -->
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>

      
      <script src="../_static/theme-vendors.js"></script>
      <script src="../_static/theme.js" defer></script>
    
      <link rel="shortcut icon" href="../_static/eq-logo-favicon.png"/>
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="Sparse and tensor grid quadrature rules" href="tutorial_8.html" />
  <link rel="prev" title="Polynomial regression" href="tutorial_6.html" /> 
  </head>

  <body>
    <div id="app" class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="/" class="home-link">
    
      <img class="logo" src="../_static/logo-5-black-text-lowres.png" alt="logo"/>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="/equadratures/"
        class="nav-link">
          Equadratures
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/eintegrator/"
        class="nav-link">
          Eintegrator
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/docs/_documentation/"
        class="nav-link">
          Documentation
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/workshops/"
        class="nav-link">
          Workshops
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/about-us/"
        class="nav-link">
          About Us
      </a>
    </div>
  


  
    <div class="nav-item">
      <a href="https://discourse.effective-quadratures.org/"
        class="nav-link external" target="_blank" >
          Discourse <outboundlink/>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/Effective-Quadratures"
        class="nav-link external" target="_blank" >
          Github <outboundlink/>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="/equadratures/"
        class="nav-link">
          Equadratures
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/eintegrator/"
        class="nav-link">
          Eintegrator
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/docs/_documentation/"
        class="nav-link">
          Documentation
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/workshops/"
        class="nav-link">
          Workshops
      </a>
    </div>
  
    <div class="nav-item">
      <a href="/about-us/"
        class="nav-link">
          About Us
      </a>
    </div>
  


  
    <div class="nav-item">
      <a href="https://discourse.effective-quadratures.org/"
        class="nav-link external" target="_blank" >
          Discourse <outboundlink/>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/Effective-Quadratures"
        class="nav-link external" target="_blank" >
          Github <outboundlink/>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="index.html#documentation">documentation</a></span>
      </p>
      <ul class="">
        
          <li class="toctree-l1 "><a href="research.html" class="reference internal ">Research: theory and practice</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="modules.html" class="reference internal ">Modules</a>

            
          </li>

        
          <li class="toctree-l1 "><a href="tutorials.html" class="reference internal ">Tutorials</a>

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
    
      <li><a href="tutorials.html">Tutorials</a> &raquo;</li>
    
    <li>Bayesian polynomial regression</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="tutorial_6.html"
       title="previous chapter">← Polynomial regression</a>
  </li>
  <li class="next">
    <a href="tutorial_8.html"
       title="next chapter">Sparse and tensor grid quadrature rules →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main">
            
  <div class="section" id="bayesian-polynomial-regression">
<h1>Bayesian polynomial regression<a class="headerlink" href="#bayesian-polynomial-regression" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial we present a Bayesian analogue to polynomial regression; the material presented here is heavily derived  from Chapter 3.8 in Rogers and Girolami. We will aim to fit a Bayesian polynomial regression model to the data we considered in the prior tutorial. Bayesian approaches are characterized by four key elements: the model, the prior, the likelihood and the posterior.</p>
<p><strong>The model</strong></p>
<p>We define our model as</p>
<div class="math notranslate nohighlight">
\[\mathbf{f} = \mathbf{Ax} + \boldsymbol{\epsilon},\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{N \times M}\)</span> is our <em>Vandermonde</em> type matrix, comprising of <span class="math notranslate nohighlight">\(M\)</span> orthogonal polynomials evaluated at the <span class="math notranslate nohighlight">\(N\)</span> <em>sample inputs</em>. In other words each entry of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> can be given by</p>
<div class="math notranslate nohighlight">
\[\mathbf{A}_{ij} = \psi_i \left(  \lambda_j \right).\]</div>
<p>Returning back to the first equation, we have <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} \in \mathbb{R}^{N}\)</span> which is the noise associated with each <em>sample output</em>, i.e., each element of the vector. To simplify matters, we introduce the standard Gaussian assumption for the noise</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\epsilon} = \mathcal{N} \left( \boldsymbol{0}, \boldsymbol{\Sigma} \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{0}\)</span> indicates the zero vector and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> denotes the covariance matrix associated with the noisy measurements. We assume that these are known. Infact we will assume that</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the identity matrix; we set <span class="math notranslate nohighlight">\(\sigma=1.5\)</span>.</p>
<p><strong>The likelihood</strong></p>
<p>In a Bayesian context, the likelihood can be interpreted as a model of some parameters given data. It is formed from the joint probability of both the same data and its associated model parameters. Our likelihood function here is given by</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) = \mathcal{N} \left( \mathbf{Ax}, \sigma^2 \mathbf{I}     \right).\]</div>
<p><strong>The prior</strong></p>
<p>One of the goals of polynomial regression is to ascertain the value of our coefficients <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. As we would like an exact expression for the posterior, as Rogers and Girolami state, we require a prior that is <strong>conjugate</strong> to the Gaussian likelihood above. The simplest choice for facilitating this conjugacy is to ensure that our prior too is Gaussian. We therefore write our prior as</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right) = \mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right),\]</div>
<p>where we set <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \boldsymbol{0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{0} = 10 \mathbf{I}\)</span>. The high-level idea is that we generate random coefficients—sampling from <span class="math notranslate nohighlight">\(\mathcal{N} \left( \boldsymbol{\mu}_{0}, \boldsymbol{\Sigma}_{0}  \right)\)</span>—for the first six Legendre polynomials and plot them!</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p_order</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">myBasis</span> <span class="o">=</span> <span class="n">Basis</span><span class="p">(</span><span class="s1">&#39;Univariate&#39;</span><span class="p">)</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">Poly</span><span class="p">(</span><span class="n">myParameters</span><span class="p">,</span> <span class="n">myBasis</span><span class="p">)</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_poly</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="c1"># Extracting the Vandermonde-type matrix!</span>

<span class="c1"># Define the prior!</span>
<span class="n">mu_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">Sigma_0</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">coefficients_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_0</span><span class="p">,</span> <span class="n">Sigma_0</span><span class="p">,</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="../_images/tutorial_7_fig_a.png"><img alt="../_images/tutorial_7_fig_a.png" src="../_images/tutorial_7_fig_a.png" style="width: 470.40000000000003px; height: 356.40000000000003px;" /></a>
<p class="caption"><span class="caption-text">Figure. Samples from the prior distribution (grey lines) assigned to the polynomial coefficients.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p><strong>The posterior</strong></p>
<p>But clearly this isn’t too informative. What we want is to posterior! The posterior can be expressed as a product of the likelihood and the prior. In this tutorial, we know our posterior will also be a Gaussian distribution. What is not known is its precise mean and covariance. Using Bayes’ rule, one can express the posterior distribution as</p>
<div class="math notranslate nohighlight">
\[p \left( \mathbf{x} | \mathbf{f}, \mathbf{A}, \sigma^2  \right) \propto  p \left( \mathbf{f} | \mathbf{x}, \mathbf{A}, \sigma^2 \right) p \left( \mathbf{x} | \boldsymbol{\mu}_{0} , \boldsymbol{\Sigma}_{0} \right),\]</div>
<p>which upon simplification yields</p>
<div class="math notranslate nohighlight">
\[= \text{exp} \left\{  -\frac{1}{2} \left( \frac{1}{\sigma^2} \left( \mathbf{f} - \mathbf{Ax}  \right)^{T}  \left( \mathbf{f} - \mathbf{Ax}  \right)  +   \left( \mathbf{x} - \boldsymbol{\mu} \right)^{T}  \boldsymbol{\Sigma}_{0}^{-1}  \left( \mathbf{x} - \boldsymbol{\mu} \right)     \right)     \right\}.\]</div>
<p>The covariance and the mean can then be given as</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma}_{\mathbf{x}} = \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{A}  + \boldsymbol{\Sigma}^{-1}_{0}    \right)^{-1}\]</div>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_{\mathbf{x}} = \boldsymbol{\Sigma}_{\mathbf{x}} \left( \frac{1}{\sigma^2} \mathbf{A}^{T} \mathbf{f} + \boldsymbol{\Sigma}_{0}^{-1} \boldsymbol{\mu}_{0}   \right)\]</div>
<p>respectively.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span> <span class="c1"># only selecting 4 sample points!</span>
<span class="n">x_use</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">values</span><span class="p">]</span>
<span class="n">y_use</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">values</span><span class="p">]</span>
<span class="n">Sigma_measurements</span> <span class="o">=</span> <span class="n">noise</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">))</span>
<span class="n">P_data</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">get_poly</span><span class="p">(</span><span class="n">x_use</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># and now computing the posterior covariance and mean...</span>
<span class="n">Sigma_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span> <span class="n">P_data</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_measurements</span><span class="p">)</span> <span class="o">@</span> <span class="n">P_data</span>  <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_0</span><span class="p">)</span> <span class="p">)</span>
<span class="n">mu_x</span> <span class="o">=</span> <span class="n">Sigma_x</span> <span class="o">@</span>   <span class="n">P_data</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sigma_measurements</span><span class="p">)</span> <span class="o">@</span> <span class="n">y_use</span>
<span class="n">coefficients_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu_x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Sigma_x</span><span class="p">,</span> <span class="p">(</span><span class="n">samples</span><span class="p">,</span><span class="n">p_order</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="../_images/tutorial_7_fig_b.png"><img alt="../_images/tutorial_7_fig_b.png" src="../_images/tutorial_7_fig_b.png" style="width: 470.40000000000003px; height: 356.40000000000003px;" /></a>
<p class="caption"><span class="caption-text">Figure. Samples created from the coefficients drawn from the posterior after observing four data points. The mean is shown in blue.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>The full source code for this tutorial can be found <a class="reference external" href="https://github.com/Effective-Quadratures/Effective-Quadratures/blob/master/source/_documentation/codes/tutorial_7.py">here.</a></p>
<p><strong>References</strong></p>
<ul class="simple">
<li><p>Rogers, S., Girolami, M. (2016). A First Course in Machine Learning. Chapman and Hall/CRC, Boca Raton, Florida, U.S.A.</p></li>
</ul>
</div>


          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="tutorial_6.html"
       title="previous chapter">← Polynomial regression</a>
  </li>
  <li class="next">
    <a href="tutorial_8.html"
       title="next chapter">Sparse and tensor grid quadrature rules →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright 2016-2020 by Effective Quadratures.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a>.
</div>
            </div>
          </div>
      </page>
    </div>
    
    
  </body>
</html>