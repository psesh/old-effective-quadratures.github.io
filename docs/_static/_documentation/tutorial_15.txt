Deep learning via polynomials
==================================
In this tutorial, we extend the generality of our model by studying neural network architectures based on polynomials. A neural network is formed by one or more *hidden layers*, which are composed of multiple nodes called *perceptrons*. A perceptron takes in a linear combination (:math:`\mathbf{w}`) of the input (:math:`\mathbf{x}`) and passes it through a non-linear transformation (:math:`p`). The latter is called the *activation function* of the perceptron. The following figure illustrates the structure of a perceptron.

.. figure:: Figures/perceptron.png
        :scale: 100 %

Common activation functions include the sigmoid function, hyperbolic tangent function, and the rectified linear unit (ReLU). In this implementation, we use orthogonal polynomials as the activation function. In practice, perceptrons are usually connected together in multiple hidden layers, similar to the illustration below:

.. figure:: Figures/nn.png
        :scale: 100 %

The number of perceptrons in each layer, and the connectivity between the perceptrons can vary in practice. In the case of a single hidden layer, the model takes the form of a "multi-ridge function" with polynomials,

.. math::

        y = p_1(\mathbf{w}_1^T \mathbf{x}) + p_2(\mathbf{w}_2^T \mathbf{x}) + ... + p_n(\mathbf{w}_n^T \mathbf{x}).


In this model, the parameters are given by the ridge directions :math:`\mathbf{w}_i` (or "weights") in each perceptron, together with the polynomial coefficients of each :math:`p_i`. Given some data :math:`(\mathbf{x}^{(i)}, t^{(i)})_{i=1}^N`, how do we fit the parameters? We can minimize the mean squared error inferred from our training data,

.. math::

        L = \sum_{i=1}^N (t^{(i)} - y(\mathbf{x}^{(i)}))^2

A wealth of optimization techniques can be found in the neural networks literature. A technique known as *error backpropagation* allows efficient calculation of gradients of the loss with respect to network parameters, which allows the use of many first-order methods. The simplest first-order method is the *steepest descent* method, which updates the weights at iteration math:`\tau` according to the following

.. math::

        \mathbf{w}_i [\tau + 1] = \mathbf{w}_i[\tau] - \eta \left.\frac{\partial L}{\partial \mathbf{w}_i}\right|_{\tau}


The polynomial coefficients are updated similarly. Here, :math:`\eta` is known as the *learning rate*, which dictates the step size at each iteration. It is a *hyperparameter* which needs to be decided manually and tuned depending on application. Other methods of optimization include:

1. Momentum-based steepest descent, where we add a fraction of the previous change in parameters to the current step:

.. math::

        \mathbf{w}_i [\tau + 1] = \mathbf{w}_i[\tau] - \eta \left.\frac{\partial L}{\partial \mathbf{w}_i}\right|_{\mathbf{w}_i, \alpha_{ij}} - \beta(\mathbf{w}_i [\tau + 1] - \mathbf{w}_i[\tau])

This introduces another hyperparameter :math:`\beta`.

2. Adaptive learning rate: When the previous change causes the loss to decrease, increase the learning rate by 10%; otherwise, decrease it by 50%. The fractional increase/decrease of the learning rate are also hyperparameters.

There are also variants of these techniques, such as stochastic gradient descent, which selects random samples to evaluate the gradient at each step. Second order methods (such as Newton-Raphson) can also be used, but backpropagation becomes more complicated, and the inversion of the Hessian can be costly.

In EQ, polynomial neural networks are implemented in the Polynet class.

**Code Implementation**

We demonstrate the polynet routines through a dataset obtained from [1], concerning the system efficiency of a fan blade parameterized by 25 variables. We are given 548 points in total. We load this data into the arrays X and Y:

.. code::

        from equadratures import *
        import numpy as np
        import matplotlib.pyplot as plt
        X = np.loadtxt('h_X.dat')
        Y = np.loadtxt('h_Y.dat')

Then, we partition the data into a training set and verification set. We will fit the data on the training set, and evaluate the goodness of fit using the verification set.

.. code::

        n_data = X.shape[0]
        train = np.random.choice(n_data, 400,replace=False)
        X_train = X[train]
        Y_train = Y[train]
        ver = np.array([i for i in range(n_data) if i not in train])
        X_ver = X[ver]
        Y_ver = Y[ver]

Now, we construct an instance of the Polynet class and call the fit method to optimize the parameters.

.. code::

        net = Polynet(X_train,Y_train,2,max_iters=30000, learning_rate=1e-4,  opt='adapt', verbose=True)
        net.fit()

In this example, we will use a single layer network; "2" refers to the number of hidden units in this layer. If we were to use, say, two layers with 3 perceptrons in each layer, we would put "[3,3]" in this argument. "max_iters" specify the number of maximum iterations to run the optimizer. Since we choose to use adaptive learning rate here, the "learning_rate" parameter specifies the *initial* learning rate, which will evolve across iterations. "verbose" lets us know the progress of the optimizer by displaying the current loss and learning rate.

After the optimizer is done, we can examine the goodness of fit. 

.. code::

        plt.figure()
        plt.scatter(Y_train,net.evaluate_fit(X_train), s=3)
        plt.scatter(Y_ver, net.evaluate_fit(X_ver),s=3)
        plt.xlabel('data')
        plt.ylabel('prediction')

.. figure:: Figures/result.png
        :scale: 100 %

The blue dots show the training data and the orange dots the verification data. Though the fit is not perfect (the cluster of points near the top of the range may have caused some difficulties for the optimizer). The verification data is fit with similar accuracy to the training data, showing that the model has not overfit.
        

**References**

.. [1] Seshadri, P., Shahpar, S., Constantine, P., Parks, G., Adams, M. Turbomachinery active subspace performance maps. Journal of Turbomachinery, 140(4), 041003. `Paper <http://turbomachinery.asmedigitalcollection.asme.org/article.aspx?articleid=2668256>`__
